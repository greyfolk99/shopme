{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from requests) (3.4)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl (97 kB)\n",
      "     ---------------------------------------- 0.0/97.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 97.1/97.1 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-3.1.0 requests-2.28.2 urllib3-1.26.14\n",
      "Collecting fake_useragent\n",
      "  Using cached fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
      "Collecting importlib-resources>=5.0\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from importlib-resources>=5.0->fake_useragent) (3.15.0)\n",
      "Installing collected packages: importlib-resources, fake_useragent\n",
      "Successfully installed fake_useragent-1.1.1 importlib-resources-5.12.0\n",
      "Collecting selenium\n",
      "  Using cached selenium-4.8.2-py3-none-any.whl (6.9 MB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Collecting sortedcontainers\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from trio~=0.17->selenium) (22.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Using cached exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shj92\\github\\greyfolk99\\shopme\\shopme\\scrapper\\venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sortedcontainers, PySocks, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed PySocks-1.7.1 async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.8.2 sortedcontainers-2.4.0 trio-0.22.0 trio-websocket-0.9.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install fake_useragent\n",
    "!pip install selenium\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting chrome options done\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import shutil\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "class Scrapper:\n",
    "    driver = None\n",
    "    chrome_data_folder = os.path.join(os.getcwd(), \"ChromeData\")\n",
    "    debug_window_pid = None\n",
    "    debug_window_port = 9222\n",
    "    options = None\n",
    "\n",
    "    def __init__(self):\n",
    "        chrome_options = {\n",
    "            '--disable-gpu': None,\n",
    "            '--disable-dev-shm-usage': None,\n",
    "            '--disable-setuid-sandbox': None,\n",
    "            '--no-first-run': None,\n",
    "            '--no-sandbox': None,\n",
    "            '--no-zygote': None,\n",
    "            '--disable-blink-features': 'AutomationControlled',\n",
    "            'start-maximized': None,\n",
    "            'disable-infobars': None,\n",
    "            '--disable-extensions': None,\n",
    "            '--remote-debugging-port': '9222',\n",
    "            'window-size': '1920x1080',\n",
    "            'lang': 'ko_KR',\n",
    "            'user-agent': ua.random\n",
    "        }\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        for option, value in chrome_options.items():\n",
    "            if value is not None:\n",
    "                self.options.add_argument(f\"{option}={value}\")\n",
    "            else:\n",
    "                self.options.add_argument(option)\n",
    "\n",
    "    def start(self):\n",
    "        if os.path.exists(self.chrome_data_folder):\n",
    "            shutil.rmtree(self.chrome_data_folder)\n",
    "            print('기존 캐쉬 삭제 완료')\n",
    "        debug_window = subprocess.Popen([\n",
    "            \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\",\n",
    "            f\"--remote-debugging-port={self.debug_window_port}\",\n",
    "            f\"--user-data-dir={self.chrome_data_folder}\"])\n",
    "        self.debug_window_pid = debug_window.pid\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\"source\": \"\"\" Object.defineProperty(navigator, 'webdriver', { get: () => undefined }) \"\"\"})\n",
    "        return self\n",
    "\n",
    "    def end(self):\n",
    "        self.driver.quit()\n",
    "        for proc in psutil.process_iter():\n",
    "            if proc.pid == self.debug_window_pid and f\"--remote-debugging-port={self.debug_window_port}\" in proc.cmdline():\n",
    "                proc.terminate()\n",
    "\n",
    "print('setting chrome options done')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 스크랩 시작"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_query = input(\"검색 쿼리를 입력하세요 : \")\n",
    "max_count = int(input(\"최대 스크랩 개수를 입력하세요(1 ~ 25개 추천) : \"))\n",
    "start_page = int(input(\"시작 페이지 번호를 입력하세요 : \"))\n",
    "print(\"검색 쿼리: \", search_query)\n",
    "print(\"최대 스크랩 개수: \", max_count)\n",
    "print(\"시작 페이지 번호: \", start_page)\n",
    "print(\"스크랩 시작 준비 완료\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import random\n",
    "import time\n",
    "\n",
    "results = []\n",
    "scrapper = Scrapper()\n",
    "driver = scrapper.start().driver\n",
    "\n",
    "def get_sub_urls(start_url, by: By, selector: str):\n",
    "    driver.get(start_url)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    # 해당 페이지 제품 링크들\n",
    "    wait.until(EC.presence_of_all_elements_located((by, selector)))\n",
    "    sub_urls = [link_element.get_attribute('href') for link_element in driver.find_elements(by, selector)]\n",
    "    print(f'found {len(sub_urls)} sub_urls')\n",
    "    return sub_urls\n",
    "\n",
    "def get_data_from(target_url):\n",
    "    try:\n",
    "        print()\n",
    "        driver.get(target_url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        print(f'[start]  {len(results)+1}번째 상품 가져오는 중....')\n",
    "\n",
    "        # 상품명\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".prod-buy-header__title\")))\n",
    "        item_name_element = driver.find_element(By.CSS_SELECTOR, \".prod-buy-header__title\")\n",
    "        item_name = item_name_element.text\n",
    "\n",
    "        print(f'상품명 : {item_name}')\n",
    "\n",
    "        # 상품 설명\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".prod-attr-item\")))\n",
    "        item_detail_elements = driver.find_elements(By.CSS_SELECTOR, \".prod-attr-item\")\n",
    "        item_details = [element.text for element in item_detail_elements]\n",
    "        print('상품 설명 : ')\n",
    "        for detail in item_details:\n",
    "            print(f'- {detail}')\n",
    "\n",
    "        # 상품 가격\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".total-price strong\")))\n",
    "        price_element = driver.find_element(By.CSS_SELECTOR, \".total-price strong\")\n",
    "        item_price = int(price_element.text.replace(\",\",\"\").replace(\"원\",\"\"))\n",
    "        print(f'가격 : {item_price}')\n",
    "\n",
    "        # 상품 이미지 urls\n",
    "        delay = random.uniform(5,10)\n",
    "        time.sleep(delay)\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".lazy-load-img\")))\n",
    "        image_elements = driver.find_elements(By.CSS_SELECTOR, \".lazy-load-img\")\n",
    "        if len(image_elements) < 2:\n",
    "            print('[failed] 이미지 개수 부족으로 실패')\n",
    "            return\n",
    "        image_urls = set([image.get_attribute('src').replace(\"48x48ex\",\"492x492ex\") for image in image_elements])\n",
    "        print(f'이미지 개수 : {len(image_urls)}')\n",
    "        for l, each_url in enumerate(image_urls):\n",
    "            print(f' {l} : {each_url}')\n",
    "            if each_url.startswith(\"data:image\"):\n",
    "                raise Exception(\"스크랩 한계치 초과, 재시작 필요\")\n",
    "\n",
    "    except TimeoutError as er:\n",
    "        print(f'[failed] {len(results)+1}번째 상품 가져오기 실패!')\n",
    "        print(f\"An error occurred: {type(er)}\")\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    result = {\n",
    "        \"item_name\": item_name,\n",
    "        \"item_detail\": \"  \\n\".join(item_details),\n",
    "        \"price\": item_price,\n",
    "        \"image_urls\": image_urls\n",
    "    }\n",
    "    results.append(result)\n",
    "    print(f'[success] {len(results)}개 완료')\n",
    "    print()\n",
    "\n",
    "# 링크 loop\n",
    "try:\n",
    "    print(f'검색 쿼리({search_query})로 스크랩 시작')\n",
    "    page = start_page\n",
    "    while len(results) < max_count:\n",
    "        start_url = f'https://www.coupang.com/np/search?component=&q={search_query}&channel=user&page={page}'\n",
    "        sub_urls = get_sub_urls(start_url, By.CSS_SELECTOR, \".search-product-link\")\n",
    "        for url in sub_urls:\n",
    "            if len(results) >= max_count:\n",
    "                break\n",
    "            get_data_from(url)\n",
    "        page += 1\n",
    "    print(\"스크랩 완료\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"스크랩 실패\")\n",
    "    print(f\"An error occurred: {type(e)}\")\n",
    "\n",
    "finally:\n",
    "    scrapper.end()\n",
    "    print(\"스크랩 종료\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 DB로 저장 (Option A)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DB 정보 입력"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "host = input(\"DB 호스트: \")\n",
    "port = input(\"DB 포트: \")\n",
    "user = input(\"DB 유저: \")\n",
    "password = input(\"DB 비밀번호: \")\n",
    "database = input(\"DB 이름: \")\n",
    "print(f\"DB 정보: {host}, {port}, {user}, {password}, {database}\")\n",
    "image_storage_path = input(\"저장 루트 경로(예:'C:\\\\Users\\\\shj92\\\\resources', 이미지는 'image\\item'에 저장됨): \")\n",
    "print(f\"저장 경로: {image_storage_path}\")\n",
    "account = input(\"계정(email): \")\n",
    "print(f\"계정: {account}\")\n",
    "print(\"세팅 완료\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import uuid\n",
    "\n",
    "# 데이터 가공해서 Item 테이블에 저장\n",
    "for i, item in enumerate(results):\n",
    "    conn = mysql.connector.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database=database\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # 변수 설정\n",
    "        created_by = account\n",
    "        created_at = datetime.now()\n",
    "        item_name = item[\"item_name\"]\n",
    "        item_detail = item[\"item_detail\"]\n",
    "        item_status = \"ON_SALE\"\n",
    "        price = item[\"price\"]\n",
    "        stock = 100\n",
    "        sold_count = 0\n",
    "        is_deleted = 0\n",
    "\n",
    "        # 변수 DB 저장\n",
    "        insert_query = \"INSERT INTO item (\" \\\n",
    "            \"created_at, \" \\\n",
    "            \"created_by, \" \\\n",
    "            \"item_detail, \" \\\n",
    "            \"item_name, \" \\\n",
    "            \"item_status, \" \\\n",
    "            \"price, \" \\\n",
    "            \"stock, \" \\\n",
    "            \"sold_count, \" \\\n",
    "            \"is_deleted) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "        cursor.execute(insert_query, (\n",
    "            created_at,\n",
    "            created_by,\n",
    "            item_detail,\n",
    "            item_name,\n",
    "            item_status,\n",
    "            price,\n",
    "            stock,\n",
    "            sold_count,\n",
    "            is_deleted))\n",
    "\n",
    "       # 마지막 ID 가져오기\n",
    "        new_item_id = cursor.lastrowid\n",
    "        print(f\"상품 {i}번째 ID : {new_item_id} \")\n",
    "\n",
    "        # 이미지 데이터 저장\n",
    "        storage_path = image_storage_path\n",
    "\n",
    "        for i, image_url in enumerate(item['image_urls']):\n",
    "\n",
    "            # 변수 저장\n",
    "            item_id = new_item_id\n",
    "            image_type = \"THUMBNAIL\" if i == 0 else \"PRODUCT\"\n",
    "            original_image_name = os.path.basename(urlparse(image_url).path)\n",
    "            image_name = f\"{str(uuid.uuid4())}_{original_image_name}\"\n",
    "            item_image_sub_path = 'image\\\\item'\n",
    "            created_at = datetime.now()\n",
    "            created_by = account\n",
    "\n",
    "            # 이미지 스토리지 저장\n",
    "            image_data = requests.get(image_url, headers={'User-Agent': ua.random}, timeout=10).content\n",
    "            image_path = os.path.join(storage_path, item_image_sub_path, image_name)\n",
    "            print(image_path)\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(image_data)\n",
    "\n",
    "            # 변수 DB 저장\n",
    "            insert_query = \"INSERT INTO item_image (\" \\\n",
    "                \"item_id, \" \\\n",
    "                \"item_image_type, \" \\\n",
    "                \"original_filename, \" \\\n",
    "                \"filename, \" \\\n",
    "                \"sub_path, \" \\\n",
    "                \"created_at, \" \\\n",
    "                \"created_by) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "            print(insert_query)\n",
    "\n",
    "            cursor.execute(insert_query, (\n",
    "                item_id,\n",
    "                image_type,\n",
    "                original_image_name,\n",
    "                image_name,\n",
    "                item_image_sub_path,\n",
    "                created_at,\n",
    "                created_by))\n",
    "\n",
    "            print(f\"이미지 {i}번째 저장\")\n",
    "            time.sleep(random.randint(3, 7))\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"commit\")\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        conn.rollback()\n",
    "        print(\"rollback the change, there was an error\")\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 CSV로 저장 (Option B)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CSV 정보 입력"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_storage_path = input(\"이미지 저장 루트 경로(예:'C:\\\\Users\\\\shj92\\\\resources', 이미지는 'image\\item'에 저장됨): \")\n",
    "print(f\"저장 경로: {image_storage_path}\")\n",
    "account = input(\"계정(email): \")\n",
    "print(f\"계정: {account}\")\n",
    "print(\"세팅 완료\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "items_df = pd.DataFrame(columns=['created_at', 'created_by', 'item_detail', 'item_name', 'item_sell_status', 'price', 'stock', 'sold_count'])\n",
    "images_df = pd.DataFrame(columns=['created_at', 'created_by', 'filename', 'root_path', 'original_filename', 'item_image_type', 'item_id'])\n",
    "\n",
    "for i, item in enumerate(results):\n",
    "\n",
    "    # 변수 설정\n",
    "    created_by = account\n",
    "    created_at = datetime.now()\n",
    "    item_name = item[\"item_name\"]\n",
    "    item_detail = item[\"item_detail\"]\n",
    "    item_status = \"ON_SALE\"\n",
    "    price = item[\"price\"]\n",
    "    stock = 100\n",
    "    sold_count = 0\n",
    "    is_deleted = 0\n",
    "\n",
    "    items_df = items_df.append(pd.DataFrame({\n",
    "        'created_at': [created_at],\n",
    "        'created_by': [created_by],\n",
    "        'item_detail': [item_detail],\n",
    "        'item_name': [item_name],\n",
    "        'item_sell_status': [item_status],\n",
    "        'price': [price],\n",
    "        'stock': [stock],\n",
    "        'sold_count': [sold_count]}),\n",
    "        ignore_index=True)\n",
    "\n",
    "    # Get the data for the images table\n",
    "    image_path = image_storage_path\n",
    "    for j, image_url in enumerate(item['image_urls']):\n",
    "\n",
    "        item_id = i\n",
    "        image_type = \"THUMBNAIL\" if j == 0 else \"PRODUCT\"\n",
    "        original_image_name = os.path.basename(urlparse(image_url).path)\n",
    "        image_name = f\"{str(uuid.uuid4())}_{original_image_name}\"\n",
    "        item_image_sub_path = 'image\\\\item'\n",
    "        created_at = datetime.now()\n",
    "        created_by = account\n",
    "        success = False # 이미지 저장 성공 여부\n",
    "\n",
    "        try:\n",
    "            image_data = requests.get(image_url, headers={'User-Agent': ua.random}, timeout=10).content\n",
    "            with open(os.path.join(image_path, image_name), 'wb') as f:\n",
    "                f.write(image_data)\n",
    "            success = True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            success = False\n",
    "\n",
    "        images_df = images_df.append(pd.DataFrame({\n",
    "            'created_at': [created_at],\n",
    "            'created_by': [created_by],\n",
    "            'filename': [image_name],\n",
    "            'sub_path': [item_image_sub_path],\n",
    "            'original_filename': [original_image_name],\n",
    "            'item_image_type': [image_type],\n",
    "            'item_id': [],\n",
    "            'success': [success],\n",
    "            'image_url': [image_url]\n",
    "        }), ignore_index=True)\n",
    "        time.sleep(random.randint(1, 5))\n",
    "\n",
    "items_df.to_excel('items.xlsx', index=False)\n",
    "images_df.to_excel('images.xlsx', index=False)\n",
    "print(\"Data saved to excel successfully.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
